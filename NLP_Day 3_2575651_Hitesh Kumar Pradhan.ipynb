{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "128eccc5",
   "metadata": {},
   "source": [
    "# NLP Practice Assignments\n",
    "    Day 3\n",
    "    1. Sentiment Analysis:\n",
    "    Problem:\n",
    "        Analyzing sentiment in customer reviews, social media comments, or product \n",
    "        feedback to determine whether the sentiment is positive, negative, or neutral.\n",
    "    Dataset:\n",
    "        IMDb movie reviews dataset\n",
    "        Twitter sentiment analysis dataset\n",
    "        Amazon product reviews dataset.\n",
    "\n",
    "    2. Text Classification:\n",
    "    Problem: \n",
    "        Categorizing text documents into predefined categories or classes, such as \n",
    "        classifying news articles into topics like sports, politics, technology, etc.\n",
    "    Dataset: \n",
    "        20 Newsgroups dataset\n",
    "        Reuters news dataset\n",
    "        spam email classification dataset.\n",
    "\n",
    "    3. Named Entity Recognition (NER):\n",
    "    Problem: \n",
    "        Identifying and classifying entities mentioned in text, such as names of persons, \n",
    "        organizations, locations, dates, etc.\n",
    "    Dataset:\n",
    "        CoNLL 2003 dataset\n",
    "        OntoNotes dataset\n",
    "        WikiNER datase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dac98e",
   "metadata": {},
   "source": [
    "# 1. Sentiment Analysis:\n",
    "    Problem:\n",
    "        Analyzing sentiment in customer reviews, social media comments, or product \n",
    "        feedback to determine whether the sentiment is positive, negative, or neutral.\n",
    "\n",
    "\n",
    "    Dataset:\n",
    "        IMDb movie reviews dataset\n",
    "        Twitter sentiment analysis dataset\n",
    "        Amazon product reviews dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfa5476f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
      "0  5.703060e+17           neutral                        1.0000   \n",
      "1  5.703010e+17          positive                        0.3486   \n",
      "2  5.703010e+17           neutral                        0.6837   \n",
      "3  5.703010e+17          negative                        1.0000   \n",
      "4  5.703010e+17          negative                        1.0000   \n",
      "\n",
      "  negativereason  negativereason_confidence         airline  \\\n",
      "0            NaN                        NaN  Virgin America   \n",
      "1            NaN                     0.0000  Virgin America   \n",
      "2            NaN                        NaN  Virgin America   \n",
      "3     Bad Flight                     0.7033  Virgin America   \n",
      "4     Can't Tell                     1.0000  Virgin America   \n",
      "\n",
      "   airline_sentiment_gold        name  negativereason_gold  retweet_count  \\\n",
      "0                     NaN     cairdin                  NaN            0.0   \n",
      "1                     NaN    jnardino                  NaN            0.0   \n",
      "2                     NaN  yvonnalynn                  NaN            0.0   \n",
      "3                     NaN    jnardino                  NaN            0.0   \n",
      "4                     NaN    jnardino                  NaN            0.0   \n",
      "\n",
      "                                                text tweet_coord  \\\n",
      "0                @VirginAmerica What @dhepburn said.         NaN   \n",
      "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
      "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
      "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
      "\n",
      "      tweet_created tweet_location               user_timezone  \n",
      "0  24-02-2015 11:35            NaN  Eastern Time (US & Canada)  \n",
      "1  24-02-2015 11:15            NaN  Pacific Time (US & Canada)  \n",
      "2  24-02-2015 11:15      Lets Play  Central Time (US & Canada)  \n",
      "3  24-02-2015 11:15            NaN  Pacific Time (US & Canada)  \n",
      "4  24-02-2015 11:14            NaN  Pacific Time (US & Canada)  \n"
     ]
    }
   ],
   "source": [
    "# here we take twitter sentimate dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the dataset file is named 'Tweets.csv'\n",
    "twitter_data = pd.read_csv('Tweets.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(twitter_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af2f828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text airline_sentiment\n",
      "0                @VirginAmerica What @dhepburn said.           neutral\n",
      "1  @VirginAmerica plus you've added commercials t...          positive\n",
      "2  @VirginAmerica I didn't today... Must mean I n...           neutral\n",
      "3  @VirginAmerica it's really aggressive to blast...          negative\n",
      "4  @VirginAmerica and it's a really big bad thing...          negative\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'text' is the column containing tweet text and 'airline_sentiment' is the sentiment label\n",
    "twitter_data = twitter_data[['text', 'airline_sentiment']]\n",
    "twitter_data.dropna(inplace=True)\n",
    "\n",
    "# Display the first few rows after preprocessing\n",
    "print(twitter_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d9a9ab",
   "metadata": {},
   "source": [
    "# Text Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b175cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0                @VirginAmerica What @dhepburn said.   \n",
      "1  @VirginAmerica plus you've added commercials t...   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...   \n",
      "3  @VirginAmerica it's really aggressive to blast...   \n",
      "4  @VirginAmerica and it's a really big bad thing...   \n",
      "\n",
      "                                        cleaned_text airline_sentiment  \n",
      "0                                               said           neutral  \n",
      "1                       plu ad commerci experi tacki          positive  \n",
      "2               today must mean need take anoth trip           neutral  \n",
      "3  realli aggress blast obnoxi entertain guest fa...          negative  \n",
      "4                               realli big bad thing          negative  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub('@[^\\s]+', '', text)  # Remove mentions\n",
    "    text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    ps = PorterStemmer()\n",
    "    text = [ps.stem(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "# Apply text preprocessing to the 'text' column\n",
    "twitter_data['cleaned_text'] = twitter_data['text'].apply(preprocess_text)\n",
    "print(twitter_data[['text', 'cleaned_text', 'airline_sentiment']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbef52b",
   "metadata": {},
   "source": [
    "# Feature Extraction (TF-IDF Vectorization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b73e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorization Completed.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming 'cleaned_text' is the preprocessed text\n",
    "X = twitter_data['cleaned_text']\n",
    "y = twitter_data['airline_sentiment']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text using TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vectorized = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "X_test_vectorized = tfidf_vectorizer.transform(X_test).toarray()\n",
    "\n",
    "print(\"TF-IDF Vectorization Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db278d",
   "metadata": {},
   "source": [
    "# Train a Sentiment Analysis Model (Naive Bayes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad4334c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Model Accuracy: 0.65\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.65      0.93      0.76        55\n",
      "     neutral       0.60      0.32      0.41        38\n",
      "    positive       0.71      0.56      0.63        27\n",
      "\n",
      "    accuracy                           0.65       120\n",
      "   macro avg       0.65      0.60      0.60       120\n",
      "weighted avg       0.65      0.65      0.62       120\n",
      "\n",
      "Sentiment Analysis Model Training Completed.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = naive_bayes_classifier.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Sentiment Analysis Model Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Sentiment Analysis Model Training Completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1d7a99",
   "metadata": {},
   "source": [
    "# Unseen Text Prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d731bb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions for Unseen Text:\n",
      "Text: Great airline experience!\n",
      "Predicted Sentiment: Positive\n",
      "\n",
      "Text: I'm not happy with the service.\n",
      "Predicted Sentiment: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example unseen text\n",
    "unseen_text = [\"Great airline experience!\",\n",
    "               \"I'm not happy with the service.\"]\n",
    "\n",
    "# Preprocess the unseen text\n",
    "cleaned_unseen_text = [preprocess_text(text) for text in unseen_text]\n",
    "\n",
    "# Transform using TF-IDF vectorizer\n",
    "unseen_text_vectorized = tfidf_vectorizer.transform(cleaned_unseen_text).toarray()\n",
    "\n",
    "# Make predictions\n",
    "predictions_unseen = naive_bayes_classifier.predict(unseen_text_vectorized)\n",
    "\n",
    "# Display predictions for unseen text:\n",
    "print(\"\\nPredictions for Unseen Text:\")\n",
    "for text, prediction in zip(unseen_text, predictions_unseen):\n",
    "    sentiment = \"Positive\" if prediction == 'positive' else \"Negative\" if prediction == 'negative' else \"Neutral\"\n",
    "    print(f\"Text: {text}\\nPredicted Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f239615",
   "metadata": {},
   "source": [
    "# 2. Text Classification:\n",
    "    Problem: \n",
    "        Categorizing text documents into predefined categories or classes, such as \n",
    "        classifying news articles into topics like sports, politics, technology, etc.\n",
    "    Dataset: \n",
    "        20 Newsgroups dataset\n",
    "        Reuters news dataset\n",
    "        spam email classification dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23fc7c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset of 20 Newsgroups Dataset\n",
      "                                                text                  category\n",
      "0  \\n\\nI am sure some bashers of Pens fans are pr...          rec.sport.hockey\n",
      "1  My brother is in the market for a high-perform...  comp.sys.ibm.pc.hardware\n",
      "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...     talk.politics.mideast\n",
      "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...  comp.sys.ibm.pc.hardware\n",
      "4  1)    I have an old Jasmine drive which I cann...     comp.sys.mac.hardware\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "# Load a subset of the 20 Newsgroups dataset (e.g., first 100 rows)\n",
    "newsgroups_data_subset = fetch_20newsgroups(subset='all', shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
    "df_newsgroups_subset = pd.DataFrame({'text': newsgroups_data_subset.data[:100], 'category': [newsgroups_data_subset.target_names[i] for i in newsgroups_data_subset.target[:100]]})\n",
    "\n",
    "# Display the first few rows of the subset\n",
    "print(\"Subset of 20 Newsgroups Dataset\")\n",
    "print(df_newsgroups_subset.head())\n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b78bc14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Preprocessing for Subset\n",
      "                                                text  \\\n",
      "0  \\n\\nI am sure some bashers of Pens fans are pr...   \n",
      "1  My brother is in the market for a high-perform...   \n",
      "2  \\n\\n\\n\\n\\tFinally you said what you dream abou...   \n",
      "3  \\nThink!\\n\\nIt's the SCSI card doing the DMA t...   \n",
      "4  1)    I have an old Jasmine drive which I cann...   \n",
      "\n",
      "                                        cleaned_text                  category  \n",
      "0  sure basher pen fan pretti confus lack kind po...          rec.sport.hockey  \n",
      "1  brother market high perform video card support...  comp.sys.ibm.pc.hardware  \n",
      "2  final said dream mediterranean new area greate...     talk.politics.mideast  \n",
      "3  think scsi card dma transfer disk scsi card dm...  comp.sys.ibm.pc.hardware  \n",
      "4  old jasmin drive cannot use new system underst...     comp.sys.mac.hardware  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Text Preprocessing for the Subset\n",
    "df_newsgroups_subset['cleaned_text'] = df_newsgroups_subset['text'].apply(preprocess_text)\n",
    "print(\"Text Preprocessing for Subset\")\n",
    "print(df_newsgroups_subset[['text', 'cleaned_text', 'category']].head())\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ec1fd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction (TF-IDF Vectorization) for Subset\n",
      "TF-IDF Vectorization Completed.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction (TF-IDF Vectorization)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Assuming 'cleaned_text' is the preprocessed text\n",
    "X_newsgroups_subset = df_newsgroups_subset['cleaned_text']\n",
    "y_newsgroups_subset = df_newsgroups_subset['category']\n",
    "\n",
    "# Vectorize the text using TF-IDF for the subset\n",
    "tfidf_vectorizer_newsgroups_subset = TfidfVectorizer(max_features=5000)\n",
    "X_vectorized_newsgroups_subset = tfidf_vectorizer_newsgroups_subset.fit_transform(X_newsgroups_subset).toarray()\n",
    "print(\"Feature Extraction (TF-IDF Vectorization) for Subset\")\n",
    "print(\"TF-IDF Vectorization Completed.\")\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a8243d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_vectorized_newsgroups_subset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Split the subset into training and testing sets\u001b[39;00m\n\u001b[0;32m      7\u001b[0m X_train_newsgroups_subset, X_test_newsgroups_subset, y_train_newsgroups_subset, y_test_newsgroups_subset \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m----> 8\u001b[0m     X_vectorized_newsgroups_subset, y_newsgroups_subset, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Train a Multinomial Naive Bayes classifier for the subset\u001b[39;00m\n\u001b[0;32m     12\u001b[0m naive_bayes_classifier_newsgroups_subset \u001b[38;5;241m=\u001b[39m MultinomialNB()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_vectorized_newsgroups_subset' is not defined"
     ]
    }
   ],
   "source": [
    "# Train a Text Classification Model (Multinomial Naive Bayes) for the Subset\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the subset into training and testing sets\n",
    "X_train_newsgroups_subset, X_test_newsgroups_subset, y_train_newsgroups_subset, y_test_newsgroups_subset = train_test_split(\n",
    "    X_vectorized_newsgroups_subset, y_newsgroups_subset, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier for the subset\n",
    "naive_bayes_classifier_newsgroups_subset = MultinomialNB()\n",
    "naive_bayes_classifier_newsgroups_subset.fit(X_train_newsgroups_subset, y_train_newsgroups_subset)\n",
    "\n",
    "# Make predictions on the test set for the subset\n",
    "y_pred_newsgroups_subset = naive_bayes_classifier_newsgroups_subset.predict(X_test_newsgroups_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9437d22",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, plot_confusion_matrix\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m learning_curve\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Plot Confusion Matrix\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plot_confusion_matrix(naive_bayes_classifier_newsgroups_subset, X_test_newsgroups_subset, y_test_newsgroups_subset, cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3a952",
   "metadata": {},
   "source": [
    "# 3. Named Entity Recognition (NER):\n",
    "    Problem: \n",
    "        Identifying and classifying entities mentioned in text, such as names of persons, \n",
    "        organizations, locations, dates, etc.\n",
    "    Dataset:\n",
    "        CoNLL 2003 dataset\n",
    "        OntoNotes dataset\n",
    "        WikiNER dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b509aa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities:\n",
      "Tesla - ORG\n",
      "Elon Musk - PERSON\n",
      "Palo Alto - GPE\n",
      "California - GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Custom text for NER\n",
    "custom_text = \"Tesla, founded by Elon Musk, is known for its electric cars. The company is headquartered in Palo Alto, California.\"\n",
    "\n",
    "# Process the custom text using spaCy\n",
    "doc = nlp(custom_text)\n",
    "\n",
    "# Extract named entities\n",
    "named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "\n",
    "# Display the named entities\n",
    "print(\"Named Entities:\")\n",
    "for entity, label in named_entities:\n",
    "    print(f\"{entity} - {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d51958ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", founded by \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Elon Musk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", is known for its electric cars. The company is headquartered in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Palo Alto\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    California\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# Visualize named entities\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "160c29c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Type Counts: Counter({'GPE': 2, 'ORG': 1, 'PERSON': 1})\n",
      "Persons: [('Elon Musk', 18, 27)]\n"
     ]
    }
   ],
   "source": [
    "named_entities = [(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents]\n",
    "from collections import Counter\n",
    "\n",
    "entity_types = [ent.label_ for ent in doc.ents]\n",
    "entity_type_counts = Counter(entity_types)\n",
    "print(\"Entity Type Counts:\", entity_type_counts)\n",
    "\n",
    "persons = [(ent.text, ent.start_char, ent.end_char) for ent in doc.ents if ent.label_ == 'PERSON']\n",
    "print(\"Persons:\", persons)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
